<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gu√≠a de Aprendizaje por Refuerzo: Q-Learning</title>
    <style>
        :root {
            --primary: #00c3ff;
            --secondary: #ff00c8;
            --tertiary: #00ff8f;
            --background: #121212;
            --text: #ffffff;
            --highlight: #ffcc00;
            --section: #1e1e1e;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background-color: var(--background);
            color: var(--text);
            line-height: 1.6;
            overflow-x: hidden;
        }
        
        header {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            padding: 2rem;
            text-align: center;
            position: relative;
            overflow: hidden;
        }
        
        .header-content {
            position: relative;
            z-index: 2;
        }
        
        h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
            background: linear-gradient(to right, var(--highlight), var(--tertiary));
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            animation: titlePulse 3s infinite;
        }
        
        @keyframes titlePulse {
            0% {
                text-shadow: 0 0 10px rgba(255, 204, 0, 0.7);
            }
            50% {
                text-shadow: 0 0 20px rgba(255, 204, 0, 0.9), 0 0 30px rgba(255, 204, 0, 0.5);
            }
            100% {
                text-shadow: 0 0 10px rgba(255, 204, 0, 0.7);
            }
        }
        
        .subtitle {
            font-size: 1.5rem;
            opacity: 0.9;
            margin-bottom: 2rem;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        .wave {
            position: absolute;
            bottom: 0;
            left: 0;
            width: 100%;
            height: 100px;
            background: var(--background);
            clip-path: polygon(100% 0%, 0% 100%, 100% 100%);
        }
        
        nav {
            background-color: rgba(0, 0, 0, 0.3);
            position: sticky;
            top: 0;
            z-index: 100;
            backdrop-filter: blur(10px);
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        .nav-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 1rem 2rem;
            max-width: 1200px;
            margin: 0 auto;
        }
        
        .logo {
            font-weight: bold;
            font-size: 1.5rem;
            color: var(--primary);
        }
        
        .nav-links {
            display: flex;
            gap: 1.5rem;
        }
        
        .nav-links a {
            color: var(--text);
            text-decoration: none;
            font-weight: 500;
            position: relative;
            padding: 0.5rem 0;
            transition: color 0.3s;
        }
        
        .nav-links a:hover {
            color: var(--highlight);
        }
        
        .nav-links a::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 0;
            height: 2px;
            background: var(--highlight);
            transition: width 0.3s;
        }
        
        .nav-links a:hover::after {
            width: 100%;
        }
        
        section {
            margin: 3rem 0;
            background-color: var(--section);
            border-radius: 10px;
            padding: 2rem;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
            transform: translateY(50px);
            opacity: 0;
            animation: fadeInUp 0.8s forwards;
        }
        
        .section-heading {
            color: var(--primary);
            font-size: 2rem;
            margin-bottom: 1.5rem;
            position: relative;
            display: inline-block;
        }
        
        .section-heading::after {
            content: '';
            position: absolute;
            left: 0;
            bottom: -10px;
            width: 50px;
            height: 4px;
            background: linear-gradient(to right, var(--primary), var(--secondary));
            border-radius: 2px;
            transition: width 0.5s;
        }
        
        section:hover .section-heading::after {
            width: 100%;
        }
        
        p {
            margin-bottom: 1rem;
        }
        
        .concept-box {
            background: linear-gradient(145deg, rgba(30, 30, 30, 0.9), rgba(20, 20, 20, 0.9));
            border-left: 4px solid var(--tertiary);
            padding: 1rem;
            margin: 1.5rem 0;
            border-radius: 5px;
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .concept-box:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 255, 143, 0.3);
        }
        
        .concept-title {
            color: var(--tertiary);
            font-weight: bold;
            margin-bottom: 0.5rem;
        }
        
        .code-block {
            background-color: rgba(0, 0, 0, 0.5);
            border-radius: 5px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
            border-left: 4px solid var(--primary);
            font-family: 'Courier New', Courier, monospace;
        }
        
        ul, ol {
            margin-left: 2rem;
            margin-bottom: 1.5rem;
        }
        
        li {
            margin-bottom: 0.5rem;
        }
        
        .subsection {
            margin: 2rem 0;
        }
        
        .subsection-heading {
            color: var(--secondary);
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }
        
        .parameter {
            display: flex;
            margin: 1rem 0;
            padding: 1rem;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 5px;
            border-left: 3px solid var(--highlight);
        }
        
        .parameter-name {
            flex: 0 0 150px;
            font-weight: bold;
            color: var(--highlight);
        }
        
        .parameter-description {
            flex: 1;
        }
        
        .experiment-card {
            background: linear-gradient(145deg, rgba(30, 30, 30, 0.9), rgba(20, 20, 20, 0.9));
            border-radius: 10px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-top: 3px solid var(--primary);
            transition: transform 0.3s;
        }
        
        .experiment-card:hover {
            transform: scale(1.02);
        }
        
        .experiment-title {
            color: var(--primary);
            font-size: 1.2rem;
            margin-bottom: 1rem;
        }
        
        .application-box {
            padding: 1.5rem;
            margin: 1rem 0;
            background: linear-gradient(to right, rgba(0, 195, 255, 0.1), rgba(0, 255, 143, 0.1));
            border-radius: 10px;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        .application-title {
            color: var(--highlight);
            font-size: 1.3rem;
            margin-bottom: 0.5rem;
        }
        
        footer {
            background: linear-gradient(135deg, var(--secondary), var(--primary));
            padding: 3rem 2rem;
            text-align: center;
            margin-top: 5rem;
            position: relative;
        }
        
        .footer-wave {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100px;
            transform: rotate(180deg);
            background: var(--background);
            clip-path: polygon(100% 0%, 0% 100%, 100% 100%);
        }
        
        .footer-content {
            position: relative;
            z-index: 2;
        }
        
        .resources {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin-top: 2rem;
        }
        
        .resource-card {
            background: rgba(0, 0, 0, 0.3);
            padding: 1.5rem;
            border-radius: 10px;
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .resource-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
        }
        
        .resource-title {
            color: var(--highlight);
            font-size: 1.2rem;
            margin-bottom: 1rem;
        }
        
        @keyframes fadeInUp {
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        @keyframes float {
            0% {
                transform: translateY(0px);
            }
            50% {
                transform: translateY(-20px);
            }
            100% {
                transform: translateY(0px);
            }
        }
        
        .floating {
            animation: float 6s ease-in-out infinite;
        }
        
        .glow {
            text-shadow: 0 0 10px var(--primary);
        }
        
        .highlight-text {
            color: var(--highlight);
            font-weight: bold;
        }
        
        .formula {
            background: rgba(0, 0, 0, 0.5);
            padding: 1rem;
            border-radius: 5px;
            text-align: center;
            font-family: 'Courier New', Courier, monospace;
            margin: 1.5rem 0;
            color: var(--tertiary);
            border: 1px dashed var(--tertiary);
        }
    </style>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Aprendizaje por Refuerzo</h1>
            <p class="subtitle">Gu√≠a definitiva de Q-Learning y sus aplicaciones</p>
        </div>
        <div class="wave"></div>
    </header>
    
    <nav>
        <div class="nav-container">
            <div class="logo">QLearn</div>
            <div class="nav-links">
                <a href="#fundamentos">Fundamentos</a>
                <a href="#algoritmo">Algoritmo</a>
                <a href="#estructura">Estructura</a>
                <a href="#parametros">Par√°metros</a>
                <a href="#experimentos">Experimentos</a>
                <a href="#mejoras">Mejoras</a>
                <a href="#aplicaciones">Aplicaciones</a>
                <a href="#recursos">Recursos</a>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <section id="fundamentos">
            <h2 class="section-heading">Conceptos Fundamentales</h2>
            
            <p>El aprendizaje por refuerzo es un paradigma de aprendizaje autom√°tico donde un agente aprende a tomar decisiones interactuando con un entorno. A diferencia del aprendizaje supervisado donde se proporcionan ejemplos etiquetados, aqu√≠ el agente aprende mediante prueba y error bas√°ndose en recompensas.</p>
            
            <div class="concept-box">
                <div class="concept-title">Componentes Esenciales</div>
                <ul>
                    <li><span class="highlight-text">Agente:</span> La entidad que toma decisiones y aprende.</li>
                    <li><span class="highlight-text">Entorno:</span> El "mundo" donde el agente opera.</li>
                    <li><span class="highlight-text">Estados (S):</span> Las diferentes situaciones en las que puede encontrarse el agente.</li>
                    <li><span class="highlight-text">Acciones (A):</span> Lo que el agente puede hacer.</li>
                    <li><span class="highlight-text">Recompensas (R):</span> El feedback num√©rico que recibe el agente despu√©s de cada acci√≥n.</li>
                    <li><span class="highlight-text">Pol√≠tica (œÄ):</span> La estrategia que sigue el agente para decidir qu√© acci√≥n tomar en cada estado.</li>
                </ul>
            </div>
            
            <p>El objetivo del aprendizaje por refuerzo es encontrar una pol√≠tica √≥ptima que maximice la recompensa acumulada a largo plazo, no solo la inmediata.</p>
        </section>
        
        <section id="algoritmo">
            <h2 class="section-heading">Algoritmo Q-Learning en Detalle</h2>
            
            <p>Q-Learning es un algoritmo de aprendizaje por refuerzo que aprende el valor de una acci√≥n en un estado particular. La "Q" se refiere a la "calidad" de una acci√≥n en un estado determinado.</p>
            
            <div class="subsection">
                <h3 class="subsection-heading">La Tabla Q</h3>
                <ul>
                    <li>Una matriz donde las filas representan estados y las columnas representan acciones.</li>
                    <li>Cada celda Q(s,a) contiene el valor esperado de tomar la acci√≥n a en el estado s.</li>
                    <li>Al principio se inicializa con valores arbitrarios (normalmente peque√±os y aleatorios).</li>
                </ul>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-heading">La Ecuaci√≥n de Actualizaci√≥n de Q</h3>
                <p>La f√≥rmula fundamental de Q-Learning:</p>
                
                <div class="formula">
                    Q(s,a) = Q(s,a) + Œ± * [R + Œ≥ * max(Q(s',a')) - Q(s,a)]
                </div>
                
                <p>Donde:</p>
                <ul>
                    <li>Q(s,a) es el valor actual</li>
                    <li>Œ± (alpha) es la tasa de aprendizaje</li>
                    <li>R es la recompensa inmediata</li>
                    <li>Œ≥ (gamma) es el factor de descuento para recompensas futuras</li>
                    <li>max(Q(s',a')) es el mejor valor para el siguiente estado s'</li>
                    <li>[R + Œ≥ * max(Q(s',a')) - Q(s,a)] es el "error de diferencia temporal"</li>
                </ul>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-heading">Exploraci√≥n vs. Explotaci√≥n</h3>
                <div class="concept-box">
                    <div class="concept-title">Balance Cr√≠tico</div>
                    <ul>
                        <li><span class="highlight-text">Exploraci√≥n:</span> Probar acciones nuevas para descubrir mejores estrategias.</li>
                        <li><span class="highlight-text">Explotaci√≥n:</span> Usar el conocimiento actual para maximizar recompensas.</li>
                    </ul>
                    <p>El equilibrio se gestiona mediante la estrategia Œµ-greedy (epsilon-greedy):</p>
                    <ul>
                        <li>Con probabilidad Œµ, elegimos una acci√≥n aleatoria (exploraci√≥n).</li>
                        <li>Con probabilidad 1-Œµ, elegimos la acci√≥n con mayor valor Q (explotaci√≥n).</li>
                        <li>El valor de Œµ normalmente decrece con el tiempo (menos exploraci√≥n al final).</li>
                    </ul>
                </div>
            </div>
        </section>
        
        <section id="estructura">
            <h2 class="section-heading">Estructura del Proyecto</h2>
            
            <div class="subsection">
                <h3 class="subsection-heading">Clase EntornoCuadricula</h3>
                <div class="concept-box">
                    <div class="concept-title">El Mundo del Agente</div>
                    <p>Representa el mundo donde opera el agente y gestiona:</p>
                    <ul>
                        <li>Posiciones (agente, meta, obst√°culos)</li>
                        <li>L√≥gica de movimiento</li>
                        <li>Sistema de recompensas</li>
                        <li>Reinicio del entorno</li>
                        <li>Visualizaci√≥n</li>
                    </ul>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-heading">Clase AgenteQLearning</h3>
                <div class="concept-box">
                    <div class="concept-title">El Cerebro del Sistema</div>
                    <p>Implementa el algoritmo Q-Learning y contiene:</p>
                    <ul>
                        <li>La tabla Q</li>
                        <li>Pol√≠tica de selecci√≥n de acciones (Œµ-greedy)</li>
                        <li>L√≥gica de aprendizaje (actualizaci√≥n de Q)</li>
                        <li>Gesti√≥n de los par√°metros de aprendizaje</li>
                    </ul>
                </div>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-heading">Funciones Auxiliares</h3>
                <ul>
                    <li><span class="highlight-text">entrenar_agente():</span> Coordina el proceso de entrenamiento.</li>
                    <li><span class="highlight-text">plot_resultados():</span> Visualiza el progreso del aprendizaje.</li>
                    <li><span class="highlight-text">mostrar_politica():</span> Representa gr√°ficamente la pol√≠tica aprendida.</li>
                    <li><span class="highlight-text">demostrar_agente():</span> Muestra el comportamiento del agente entrenado.</li>
                </ul>
            </div>
        </section>
        
        <section id="parametros">
            <h2 class="section-heading">Par√°metros Clave y su Impacto</h2>
            
            <div class="parameter">
                <div class="parameter-name">Tasa de Aprendizaje (Œ±, alpha)</div>
                <div class="parameter-description">
                    <p><span class="highlight-text">Valor bajo (0.01-0.1):</span> Aprendizaje lento pero estable. Bueno para entornos deterministas.</p>
                    <p><span class="highlight-text">Valor alto (0.5-0.9):</span> Aprendizaje r√°pido pero potencialmente inestable. Puede ser √∫til al principio.</p>
                    <p><span class="highlight-text">Impacto:</span> Controla cu√°nto influyen las nuevas experiencias en el conocimiento actual.</p>
                </div>
            </div>
            
            <div class="parameter">
                <div class="parameter-name">Factor de Descuento (Œ≥, gamma)</div>
                <div class="parameter-description">
                    <p><span class="highlight-text">Valor bajo (0.1-0.5):</span> Prioriza recompensas inmediatas. Agente "miope".</p>
                    <p><span class="highlight-text">Valor alto (0.9-0.99):</span> Valora recompensas futuras. Agente "con visi√≥n de futuro".</p>
                    <p><span class="highlight-text">Impacto:</span> Define cu√°nto importan las recompensas futuras versus las inmediatas.</p>
                </div>
            </div>
            
            <div class="parameter">
                <div class="parameter-name">Epsilon Inicial y Decaimiento</div>
                <div class="parameter-description">
                    <p><span class="highlight-text">Epsilon inicial alto (0.9-1.0):</span> Mucha exploraci√≥n al principio.</p>
                    <p><span class="highlight-text">Decaimiento lento (0.99-0.999):</span> Exploraci√≥n sostenida por m√°s tiempo.</p>
                    <p><span class="highlight-text">Impacto:</span> Controla el balance exploraci√≥n-explotaci√≥n a lo largo del tiempo.</p>
                </div>
            </div>
            
            <div class="parameter">
                <div class="parameter-name">Esquema de Recompensas</div>
                <div class="parameter-description">
                    <p><span class="highlight-text">Recompensas espaciadas:</span> Problema del cr√©dito retrasado, aprendizaje m√°s dif√≠cil.</p>
                    <p><span class="highlight-text">Recompensas frecuentes:</span> Aprendizaje m√°s r√°pido pero puede llevar a √≥ptimos locales.</p>
                    <p><span class="highlight-text">Impacto:</span> Define qu√© comportamientos se refuerzan y cu√°n dif√≠cil es el aprendizaje.</p>
                </div>
            </div>
        </section>
        
        <section id="experimentos">
            <h2 class="section-heading">Experimentos Sugeridos</h2>
            
            <div class="experiment-card floating">
                <h3 class="experiment-title">Modificaci√≥n de Par√°metros</h3>
                <ul>
                    <li>Prueba diferentes valores de Œ±, Œ≥ y Œµ.</li>
                    <li>Registra c√≥mo afecta a la velocidad de aprendizaje y la pol√≠tica final.</li>
                    <li>Compara las curvas de aprendizaje.</li>
                </ul>
            </div>
            
            <div class="experiment-card">
                <h3 class="experiment-title">Alteraci√≥n del Entorno</h3>
                <ul>
                    <li>Cambia el tama√±o de la cuadr√≠cula.</li>
                    <li>Modifica la disposici√≥n de los obst√°culos.</li>
                    <li>A√±ade m√∫ltiples metas con diferentes recompensas.</li>
                    <li>Crea "zonas de peligro" con peque√±as penalizaciones.</li>
                </ul>
            </div>
            
            <div class="experiment-card floating">
                <h3 class="experiment-title">Modificaci√≥n del Esquema de Recompensas</h3>
                <ul>
                    <li>Prueba con recompensas en funci√≥n de la distancia a la meta.</li>
                    <li>Implementa recompensas negativas por cada paso (para incentivar rutas cortas).</li>
                    <li>Introduce recompensas por explorar nuevas √°reas.</li>
                </ul>
            </div>
            
            <div class="experiment-card">
                <h3 class="experiment-title">Comparaci√≥n con Otros Algoritmos</h3>
                <ul>
                    <li>Implementa SARSA y compara con Q-Learning.</li>
                    <li>Prueba versiones con memoria (considerar √∫ltimos n estados).</li>
                    <li>Experimenta con actualizaci√≥n doble de Q (para reducir sesgo optimista).</li>
                </ul>
            </div>
        </section>
        
        <section id="mejoras">
            <h2 class="section-heading">Mejoras y Expansiones</h2>
            
            <div class="subsection">
                <h3 class="subsection-heading">Mejoras Algor√≠tmicas</h3>
                <ol>
                    <li><span class="highlight-text">Q-Learning con Elegibilidad:</span> Propaga la actualizaci√≥n a estados visitados recientemente.</li>
                    <li><span class="highlight-text">Deep Q-Learning:</span> Usa redes neuronales para aproximar la funci√≥n Q (√∫til para espacios de estados grandes).</li>
                    <li><span class="highlight-text">Prioritized Experience Replay:</span> Da prioridad a experiencias con mayor error de TD.</li>
                    <li><span class="highlight-text">Double Q-Learning:</span> Reduce sobreestimaci√≥n usando dos tablas Q.</li>
                </ol>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-heading">Expansiones del Entorno</h3>
                <ol>
                    <li><span class="highlight-text">Entornos Din√°micos:</span> Obst√°culos que se mueven o aparecen/desaparecen.</li>
                    <li><span class="highlight-text">Informaci√≥n Parcial:</span> El agente solo ve una parte del entorno.</li>
                    <li><span class="highlight-text">Multiagente:</span> Varios agentes interactuando en el mismo entorno.</li>
                    <li><span class="highlight-text">Entornos 3D:</span> A√±adir una dimensi√≥n m√°s para navegaci√≥n.</li>
                </ol>
            </div>
            
            <div class="subsection">
                <h3 class="subsection-heading">Mejoras en la Implementaci√≥n</h3>
                <ol>
                    <li><span class="highlight-text">Paralelizaci√≥n:</span> Entrenar m√∫ltiples agentes simult√°neamente.</li>
                    <li><span class="highlight-text">Interfaz Gr√°fica:</span> Crear una UI para facilitar experimentaci√≥n.</li>
                    <li><span class="highlight-text">Persistencia:</span> Guardar/cargar modelos entrenados.</li>
                    <li><span class="highlight-text">Telemetr√≠a:</span> A√±adir m√©tricas detalladas sobre el aprendizaje.</li>
                </ol>
            </div>
        </section>
        
        <section id="aplicaciones">
            <h2 class="section-heading">Ejemplos de Aplicaciones Reales</h2>
            
            <div class="application-box">
                <h3 class="application-title">Rob√≥tica</h3>
                <ul>
                    <li><span class="highlight-text">Navegaci√≥n aut√≥noma:</span> Robots que aprenden a moverse en entornos din√°micos.</li>
                    <li><span class="highlight-text">Manipulaci√≥n:</span> Brazos rob√≥ticos que aprenden a coger objetos.</li>
                </ul>
            </div>
            
            <div class="application-box">
                <h3 class="application-title">Videojuegos</h3>
                <ul>
                    <li><span class="highlight-text">NPCs adaptativos:</span> Personajes que aprenden patrones de juego.</li>
                    <li><span class="highlight-text">Balanceo autom√°tico:</span> Ajuste de dificultad basado en rendimiento del jugador.</li>
                </ul>
            </div>
            
            <div class="application-box">
                <h3 class="application-title">Optimizaci√≥n de Sistemas</h3>
                <ul>
                    <li><span class="highlight-text">Control de tr√°fico:</span> Gesti√≥n din√°mica de sem√°foros.</li>
                    <li><span class="highlight-text">Asignaci√≥n de recursos:</span> Distribuci√≥n eficiente en sistemas inform√°ticos.</li>
                </ul>
            </div>
            
            <div class="application-box">
                <h3 class="application-title">Finanzas</h3>
                <ul>
                    <li><span class="highlight-text">Trading algor√≠tmico:</span> Estrategias de compra/venta adaptativas.</li>
                    <li><span class="highlight-text">Gesti√≥n de riesgos:</span> Decisiones √≥ptimas bajo incertidumbre.</li>
                </ul>
            </div>
            
            <div class="application-box">
                <h3 class="application-title">Medicina</h3>
                <ul>
                    <li><span class="highlight-text">Dosificaci√≥n personalizada:</span> Ajuste de tratamientos.</li>
                    <li><span class="highlight-text">Sistemas de diagn√≥stico:</span> Apoyo a decisiones m√©dicas.</li>
                </ul>
            </div>
        </section>
        
        <section id="recursos">
            <h2 class="section-heading">Recursos Adicionales</h2>
            
            <div class="resources">
                <div class="resource-card">
                    <h3 class="resource-title">Libros</h3>
                    <ul>
                        <li>"Reinforcement Learning: An Introduction" por Richard S. Sutton y Andrew G. Barto</li>
                        <li>"Deep Reinforcement Learning Hands-On" por Maxim Lapan</li>
                        <li>"Algorithms for Reinforcement Learning" por Csaba Szepesv√°ri</li>
                    </ul>
                </div>
                
                <div class="resource-card">
                    <h3 class="resource-title">Cursos Online</h3>
                    <ul>
                        <li>CS234: Reinforcement Learning (Stanford)</li>
                        <li>Coursera: Reinforcement Learning Specialization (University of Alberta)</li>
                        <li>David Silver's Reinfor
